{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing Notebook\n",
    "This template script contains all the code needed to import and process Raman spectra, then save them in a standardised file format. It can import and process a single file, or multiple files at once. Available processing includes:\n",
    "- Averaging for multi-spec measurements\n",
    "- Baseline (background) subtraction\n",
    "- Reference spectrum subtraction\n",
    "- Automatic peak detection\n",
    "- Automatic peak fitting\n",
    "\n",
    "# Requires input data files to be organised as follows:\n",
    "- Location: {Data_dir}/{Sample}/{Measurement Date}/\n",
    "- Sample/Measurement metadata must be recorded in either file {spec filename}_MetaData.csv, or within spec filename\n",
    "    - Spec filename format:\n",
    "    \n",
    "        {Spec ID number}\\_{Sample}\\_{Subsample}\\_{notes}\\_{Magnification}x\\_{Laser wavelength}nm\\_{Laser power}mW\\_{Accumulations}x{Exposure time}.txt\n",
    "    \n",
    "    - Each spectrum file in a given project must have a unique identifier (preferable a sequential ID number) so that it can always be distinguished from other measurements, even when they are otherwise identical. These ID numbers can be used to refer to particular spectra when manually specifying settings to use for outliers/exceptional cases.\n",
    "    - any pre-processing steps applied to the data should be included at the end of the filename\n",
    "        - C: cosmic ray removal\n",
    "        - O: outlier removal\n",
    "        - N: normalisation\n",
    "        \n",
    "# How to use this script:\n",
    "- Pick your directories and specify what sample/setting filters you want to apply when importing files\n",
    "- Adjust the data import string to make sure it matches your file structure and naming conventions (if necessary)\n",
    "- Each section of code starts with a set of user input variables to control what it does:\n",
    "\n",
    "             skip: skip this section entirely (True/False)\n",
    "            debug: print debug messages during execution (True/False)\n",
    "        show_plot: show plotted figures in viewer (True/False)\n",
    "        save_plot: save plotted figures to file (True/False)\n",
    "        \n",
    "    Data processing sections also need to be told what x,y values and keynames to use for input/output:\n",
    "    \n",
    "            x_key: the keyname to use as x values (e.g. 'raman_shift')\n",
    "            y_key: the keyname to use as y values (e.g. 'y_av_sub')\n",
    "          alt_key: the back-up keyname to use as y values if y_key does not exist for a given measurement (e.g. 'y_av')\n",
    "          new_key: the keyname to save the output under (e.g. 'y_av_refsub' or 'fitted_peaks')\n",
    "          \n",
    "    Some processing steps also have manual overrides, for example if you want to specify what measurements need to have a reference spectrum subtracted, or which peak positions to fit for a given measurement. Please refer to those sections.\n",
    "\n",
    "- To turn off/on sections of code, e.g. peak detection, simply use the 'skip' variable at the start of each section.\n",
    "- When rerunning sections of code, bear in mind that data and variables will reflect the most recent state and may give you unexpected results - sometimes it's safer to rerun the whole script. \n",
    "- When you are happy with the output of a particular section, you can switch the 'debug' variable to False to hide debug text generated in the window. It will continue to show key information, and plots.\n",
    "- To save time and memory, you can switch 'show_plot' to False to stop plots being rendered in the viewer. They will still be saved to disk if 'save_plot' is True.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# define where your data is, and what files to import\n",
    "\n",
    "# list directories for input data, figures and output files\n",
    "Data_dir = '../Raman data/'\n",
    "Fig_dir = '../Raman figures/'\n",
    "Out_dir = '../Raman output/'\n",
    "Ref_dir = '../Raman data/References/'\n",
    "\n",
    "Technique = 'Raman'         # 'Raman' or 'FTIR'\n",
    "\n",
    "# filter data import by sample / subsample\n",
    "Sample = '*'                # name of sample, or '*' for all\n",
    "Subsample = '*'             # name of subsample, or '*'\n",
    "Measurement_ID = '*'        # unique ID of measurement, or '*'\n",
    "\n",
    "# list spectral IDs to skip when importing data, as strings\n",
    "Do_Not_Import = []\n",
    "\n",
    "# filter by measurement settings\n",
    "Metadata_File = False       # set to True to import metadata file, or False to extract data from spec filename\n",
    "Measurement_Date = '*'      # Date in YYYY-MM-DD format as string, or '*'\n",
    "Laser_Wavelength = '*'      # wavelength (in nm) as integer, or '*'\n",
    "Laser_Power = '*'           # power (in mW or %) as int/float, or '*'\n",
    "Exposure_Time = '*'         # exposure time (in sec) as int/float, or '*'\n",
    "Accumulations = '*'         # accumulations as int, or '*'\n",
    "Magnification = '*'         # objective magnification as int, or '*'\n",
    "Preprocessing = '*'         # specify required preprocessing, or '*' for best available"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==================================================\n",
    "# this section imports necessary python modules\n",
    "\n",
    "import os\n",
    "import math\n",
    "import glob\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import lmfit as lmfit\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.signal import argrelextrema\n",
    "from scipy.signal import savgol_filter\n",
    "\n",
    "# import VibSpec module functions\n",
    "from functions_v02 import *\n",
    "\n",
    "\"\"\"\n",
    "# ==================================================\n",
    "# FILE SEARCH\n",
    "# - this section searches for spectrum files that match the specified settings\n",
    "# ==================================================\n",
    "\"\"\"\n",
    "\n",
    "print(\"SEARCHING FOR SPECTRUM DATA FILES...\")\n",
    "\n",
    "# find files\n",
    "#   - currently requires data files to be named and organised as follows:\n",
    "# Data_dir/{Sample}/{Measurement_Date}/{Measurement_ID}_{Sample}_{Subsample}_{Measurement Parameters}.txt\n",
    "text = \"%s%s/%s/%s_%s_%s*_%sX_%snm_%smW_%sx%ss*txt\" % (Data_dir, Sample, Measurement_Date,\n",
    "            Measurement_ID, Sample, Subsample, Magnification,\n",
    "            Laser_Wavelength, Laser_Power, Accumulations, Exposure_Time)\n",
    "print(text)\n",
    "spec_dirs = sorted(glob.glob(text))\n",
    "\n",
    "# trim to measurement ID numbers\n",
    "Spec_IDs = np.unique([spec.split(\"/\")[-1].split(\"_\")[0] for spec in spec_dirs])\n",
    "print()\n",
    "print(\"spectrum IDs found:\", Spec_IDs)\n",
    "\n",
    "# find appropriate pre-processed file for each measurement\n",
    "spec_dirs = []\n",
    "for ID in Spec_IDs:\n",
    "    if Preprocessing != '*':\n",
    "        # use specified preprocessing\n",
    "        text = \"%s%s*/%s/%s_*_%s.txt\" % (Data_dir, Sample, Measurement_Date, ID, Preprocessing)\n",
    "        temp = sorted(glob.glob(text))\n",
    "        if len(temp) > 0:\n",
    "            lengths = [len(s) for s in temp]\n",
    "            spec_dirs.append(temp[np.argmax(lengths)])\n",
    "    else:\n",
    "        # use best available\n",
    "        text = \"%s%s*/%s/%s_*.txt\" % (Data_dir, Sample, Measurement_Date, ID)\n",
    "        temp = sorted(glob.glob(text))\n",
    "        if len(temp) > 0:\n",
    "            lengths = [len(s) for s in temp]\n",
    "            spec_dirs.append(temp[np.argmax(lengths)])\n",
    "\n",
    "print()\n",
    "print(\"data files found:\", len(spec_dirs))\n",
    "for file in spec_dirs:\n",
    "    print(\"    \", file.split(\"/\")[-1])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data import\n",
    "\n",
    "- this section actually imports data files and extracts their metadata\n",
    "- metadata can be in a separate CSV file with the same name plus '\\_metadata', or can be extracted from the data file's name\n",
    "- measurements are imported file by file, and added sequentially to the 'data' dict\n",
    "- to access a specific measurement, you call data\\[measurement ID\\]\\[key\\], where 'key' is the type of data you want from it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"IMPORTING DATA...\")\n",
    "\n",
    "# print debug messages?\n",
    "debug = True\n",
    "\n",
    "# set up data storage dictionary\n",
    "data = {}\n",
    "\n",
    "# ==================================================\n",
    "# each measurement imported will be added to this dictionary as a Measurement object\n",
    "# To access a particular measurement, use data[measurement ID]\n",
    "\n",
    "# for each detected file\n",
    "for spec_dir in spec_dirs:\n",
    "    while True:\n",
    "        try:\n",
    "            filename = spec_dir.split(\"/\")[-1][:-4]\n",
    "            ID = filename.split(\"_\")[0]\n",
    "            if ID in Do_Not_Import:\n",
    "                print()\n",
    "                print(\" measurement %s is in Do_Not_Import list, skipping\" % filename)\n",
    "                break\n",
    "            else:\n",
    "                print()\n",
    "                print(\"importing %s\" % filename)\n",
    "                # extract sample/measurement metadata\n",
    "                metadata = False\n",
    "                if Metadata_File == True:\n",
    "                    # search for metadata file with matching name\n",
    "                    metadata_dir = glob.glob(\"%s_metadata.csv\" % spec_dir[:-4])\n",
    "                    print(\"    metadata files found:\", len(metadata_dir))\n",
    "                    if len(metadata_dir) > 0:\n",
    "                        metadata = True\n",
    "                if metadata == True:\n",
    "                    # get metadata from metadata file\n",
    "                    metadata = pd.read_csv(metadata_dir)\n",
    "                else:\n",
    "                    # get metadata from filename instead\n",
    "                    date = datetime.datetime.strptime(spec_dir.split(\"/\")[-2], \"%Y-%m-%d\")\n",
    "                    filename_split = filename.split(\"_\")\n",
    "                    # get sample info\n",
    "                    ID, sample, subsample = filename_split[:3]\n",
    "                    # check for preprocessing\n",
    "                    if len(filename_split[-1].split(\"x\")) == 2 and filename_split[-1][-1] == 's':\n",
    "                        # final item in filename is {Accumulations}x{Exposure_Time}s, no preprocessing\n",
    "                        preprocessing = 'none'\n",
    "                        # check for additional sample notes\n",
    "                        notes = ''\n",
    "                        if filename_split[2] != filename_split[-5]:\n",
    "                            notes = \"_\".join(filename_split[3:-4])\n",
    "                        mag, laser_wavelength, laser_power, accumxexp = filename_split[-4:]\n",
    "                    else:\n",
    "                        # final item in filename assumed to be preprocessing steps\n",
    "                        preprocessing = filename_split[-1]\n",
    "                        # check for additional sample notes\n",
    "                        notes = ''\n",
    "                        if filename_split[2] != filename_split[-6]:\n",
    "                            notes = \"_\".join(filename_split[3:-5])\n",
    "                            metadata_start = 4\n",
    "                        mag, laser_wavelength, laser_power, accumxexp = filename_split[-5:-1]\n",
    "                    mag = mag[:-1]  # remove 'X' from magnification\n",
    "                    laser_wavelength = int(laser_wavelength[:-2])   # remove 'nm'\n",
    "                    if laser_power[-2:] == 'mW':\n",
    "                        # laser power in mW\n",
    "                        laser_power = float(laser_power[:-2])   # remove 'mW'\n",
    "                        power_unit = 'mW'\n",
    "                    else:\n",
    "                        # assume power in %\n",
    "                        laser_power = float(laser_power[:-1])   # remove '%'\n",
    "                        power_unit = \"%%\"\n",
    "                    accumulations, exposure_time = accumxexp.split(\"x\")\n",
    "                    accumulations = int(accumulations)\n",
    "                    exposure_time = float(exposure_time[:-1])   # remove 's'\n",
    "                # report metadata\n",
    "                if debug == True:\n",
    "                    print(\"    measurement ID:\", ID)\n",
    "                    print(\"              sample:\", sample)\n",
    "                    print(\"           subsample:\", subsample)\n",
    "                    print(\"         measured on:\", date.strftime(\"%Y-%m-%d\"))\n",
    "                    print(\"               notes:\", notes)\n",
    "                    print(\"    measurement settings:\")\n",
    "                    print(\"         magnification: %s X\" % (mag))\n",
    "                    print(\"            wavelength: %s nm\" % (laser_wavelength))\n",
    "                    print(\"                 power: %s %s\" % (laser_power, power_unit))\n",
    "                    print(\"              exposure: %s x %s seconds\" % (accumulations, exposure_time))\n",
    "                    print(\"    preprocessing: %s\" % (preprocessing))\n",
    "                # import spectrum file (assumes Renishaw file formatting with either 2 or 4 columns)\n",
    "                spec = np.genfromtxt(spec_dir).transpose()\n",
    "                if debug == True:\n",
    "                    print(\"    spec array:\", np.shape(spec))\n",
    "                distances = []\n",
    "                xy_coords = []\n",
    "                if np.size(spec, axis=0) == 4:\n",
    "                    # map or line, columns=(x_position, y_position, raman_shift, intensity)\n",
    "                    spec_type = 'map'\n",
    "                    # determine splits between stacked spectra\n",
    "                    splits = np.ravel(np.where(np.roll(spec[2], 1) < spec[2]))\n",
    "                    points = len(splits)\n",
    "                    print(\"    spectral map, %s points\" % (points))\n",
    "                    # get X,Y coordinates (in um) for each point spectrum\n",
    "                    x_pos = np.asarray(spec[0][splits])\n",
    "                    y_pos = np.asarray(spec[1][splits])\n",
    "                    xy_coords = np.asarray([x_pos, y_pos])\n",
    "                    # determine if line, or map by looking at signs of dX, dY\n",
    "                    dx = np.roll(x_pos, 1)[1:] - x_pos[1:]\n",
    "                    dy = np.roll(y_pos, 1)[1:] - y_pos[1:]\n",
    "                    dx_sign = np.sign(dx)\n",
    "                    dy_sign = np.sign(dy)\n",
    "                    if np.all(dx_sign == np.sign(np.mean(dx))) & np.all(dy_sign == np.sign(np.mean(dy))):\n",
    "                        # checks if points follow a line\n",
    "                        print(\"    measurement is a 1D line scan\")\n",
    "                        spec_type = 'line'\n",
    "                        distances = np.cumsum(np.sqrt(dx**2 + dy**2))\n",
    "                        distances = np.insert(distances, 0, 0)\n",
    "                    else:\n",
    "                        # assumes points are arranged in a grid\n",
    "                        print(\"    measurement is a 2D map scan\")\n",
    "                    # extract shift, intensity spectrum for each point\n",
    "                    x = np.asarray(np.split(spec[2], splits[1:]))[0]\n",
    "                    y = np.asarray(np.split(spec[3], splits[1:]))\n",
    "                else:\n",
    "                    # single point, columns=(raman_shift, intensity)\n",
    "                    spec_type = 'point'\n",
    "                    points = 1\n",
    "                    xy_coords = np.asarray([[0],[0]])\n",
    "                    print(\"        single point measurement\")\n",
    "                    x = spec[0]\n",
    "                    y = spec[1:]\n",
    "                sort = np.argsort(x)\n",
    "                x = x[sort]\n",
    "                y = y.transpose()[sort,:]\n",
    "                infcheck = np.any(np.isinf(y), axis=1)\n",
    "                nancheck = np.any(np.isnan(y), axis=1)\n",
    "                zerocheck = np.any(y == 0, axis=1)\n",
    "                check = np.logical_or.reduce((infcheck, nancheck, zerocheck))\n",
    "                if debug == True:\n",
    "                    print(\"              shift:\", np.shape(x))\n",
    "                    print(\"          intensity:\", np.shape(y))\n",
    "                    print(\"        shift range: %0.f - %0.f cm-1\" % (np.amin(x), np.amax(x)))\n",
    "                    print(\"        inf check: %0.f/%0.f\" % (np.count_nonzero(infcheck),\n",
    "                                                               np.size(infcheck)))\n",
    "                    print(\"        nan check: %0.f/%0.f\" % (np.count_nonzero(nancheck),\n",
    "                                                               np.size(nancheck)))\n",
    "                    print(\"        zero check: %0.f/%0.f\" % (np.count_nonzero(zerocheck),\n",
    "                                                               np.size(zerocheck)))\n",
    "                if np.count_nonzero(check) > 0.5 * np.size(check):\n",
    "                    raise Exception(\"%0.f%% of spectrum data is 0/nan/inf! Cannot import!\" % (100*np.count_nonzero(check)/np.size(check)))\n",
    "                # get average spectrum (for single points, just use spec)\n",
    "                y_av = np.mean(y, axis=0)\n",
    "                # generate sample title for consistent naming\n",
    "                title = \"%s_%s_%s\" % (ID, sample, subsample)\n",
    "                if notes != '':\n",
    "                    title += \"_\" + notes\n",
    "                # create Measurement instance from imported data\n",
    "                data[str(ID)] = Measurement(\n",
    "                    ID=str(ID),\n",
    "                    title=title,\n",
    "                    filename=filename,\n",
    "                    sample=sample,\n",
    "                    subsample=subsample,\n",
    "                    notes=notes,\n",
    "                    x=x[~check],\n",
    "                    y=y[~check],\n",
    "                    ykey='y',\n",
    "                    technique='Raman',\n",
    "                    generate_average=True,\n",
    "                    laser_wavelength=laser_wavelength,\n",
    "                    spec_type=spec_type,\n",
    "                    points=points,\n",
    "                    magnification=mag,\n",
    "                    laser_power=laser_power,\n",
    "                    accumulations=accumulations,\n",
    "                    exposure_time=exposure_time,\n",
    "                    x_coords=xy_coords[0],\n",
    "                    y_coords=xy_coords[1],\n",
    "                    Fig_dir = Fig_dir,\n",
    "                    Out_dir = Out_dir\n",
    "                )\n",
    "                \n",
    "                print(\"    imported successfully!\")\n",
    "                break\n",
    "        except Exception as e:\n",
    "            print(\"    something went wrong! Exception:\", e)\n",
    "            break\n",
    "        \n",
    "print()\n",
    "print(\"%s/%s files imported\" % (len(data.keys()), len(spec_dirs)))\n",
    "\n",
    "# report which files were imported and which were not\n",
    "print()\n",
    "print(\"Files found / imported\")\n",
    "for num in Spec_IDs:\n",
    "    if num in data.keys():\n",
    "        print(\"    \", num, u'\\u2713', data[num]['filename'])\n",
    "    else:\n",
    "        text = ''\n",
    "        if num in Do_Not_Import:\n",
    "            text = 'ID in Do_Not_Import list'\n",
    "        print(\"    \", num, 'X', text)\n",
    "\n",
    "# update list of Spec IDs to only include imported spectra\n",
    "Spec_IDs = list(data.keys())\n",
    "\n",
    "samples = np.unique([data[num]['sample'] for num in Spec_IDs])\n",
    "print()\n",
    "print(\"samples in dataset:\")\n",
    "for sample in samples:\n",
    "    print(\"    \", sample)\n",
    "\n",
    "lasers = np.unique([measurement.laser_wavelength for ID, measurement in data.items()])\n",
    "print()\n",
    "print(\"laser wavelengths in dataset:\")\n",
    "for laser in lasers:\n",
    "    print(\"    \", laser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline Subtraction\n",
    "- baseline is a polynomial fitted to specified points along the x axis\n",
    "- you can specify the points and polynomial order used for each wavelength, and add additional logic for specific samples if needed\n",
    "- subtract_baseline() function finds the local average for each point Â±25 cm-1, this value can be changed using the window argument\n",
    "- automatically baselines each spectrum in a multi-spec measurement separately, individual spectra are stored in y_sub\\[measurement index\\]\\[spec index\\]\n",
    "- y_av_sub is the baselined average spectrum\n",
    "- y_sub_av is the average of the baselined spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip this section?\n",
    "skip = False\n",
    "\n",
    "# print debug messages in viewer?\n",
    "debug = True\n",
    "\n",
    "# keyname of x value parameter to use ('raman_shift', 'wavelength', 'frequency')\n",
    "x_key = 'raman_shift'\n",
    "\n",
    "# keyname of y value parameter to use ('y_av')\n",
    "y_key = 'y_av'\n",
    "\n",
    "# new keyname for baselined spectrum\n",
    "new_key = 'y_av_sub'\n",
    "\n",
    "# x positions to use for fitting baseline\n",
    "base_list = []\n",
    "\n",
    "# show plots in viewer?\n",
    "show_plot = True\n",
    "\n",
    "# save plots to file?\n",
    "save_plot = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if skip == True:\n",
    "    print(\"SKIPPING BASELINE SUBTRACTION\")\n",
    "    print(\"    (not recommended for Raman spectra)\")\n",
    "else:\n",
    "    print(\"DOING BASELINE SUBTRACTION\")\n",
    "\n",
    "    process_count = 1\n",
    "    for ID, measurement in data.items():\n",
    "        print()\n",
    "        print(\"%s/%s subtracting baseline for %s\" % (process_count, len(data.keys()), measurement.title))\n",
    "\n",
    "        # decide which set of baseline points to use\n",
    "        if measurement.laser_wavelength == 532:\n",
    "            base_list = [200, 300, 400, 415, 500, 600, 700, 800, 900, 1050, 1150, 1400, 1600,\n",
    "                         1700, 1800, 2000, 2100, 2300, 2500, 2700, 2950, 3100, 3200, 3700, 3800, 3900, 4000]\n",
    "            order = 11\n",
    "        elif measurement.laser_wavelength in [633, 638]:\n",
    "            base_list = [100, 200, 300, 400, 415, 500, 600, 700, 800, 900, 1050, 1150, 1400, 1500]\n",
    "            order = 21\n",
    "        elif measurement.laser_wavelength == 785:\n",
    "            if ID in ['R0008', 'R0009', '0010', '0012', '0013', '0014', '0015', '0026', '0027',\n",
    "                      '0028', '0028A', '0029', '0030', '0032', '0037', '0038']:\n",
    "                # these spectra earmarked for having glass PL peak, requires reference subtraction \n",
    "                base_list = [250, 475, 525, 660, 790, 900, 1000, 2100, 2250, 2500, 2600, 2950, 3100,\n",
    "                             3500, 3600, 3800]\n",
    "                order = 11\n",
    "            elif ID in ['R0010', 'R0011', 'R0012']:\n",
    "                # these spectra have resin peaks\n",
    "                base_list = [250, 475, 525, 660, 790, 1000, 1400, 1800, 2100, 2250, 2500, 2600, 2700,\n",
    "                             3100, 3500, 3600, 3800]\n",
    "                order = 11\n",
    "            else:\n",
    "                base_list = [250, 350, 475, 525, 660, 790, 900, 950, 1000, 1200, 1300, 1700, 1900, 2000, 2250, 2500,\n",
    "                             2600, 2950, 3100, 3500, 3600, 3800]\n",
    "                order = 11\n",
    "        else:\n",
    "            base_list = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1050, 1150, 1400, 1500]\n",
    "            order = 11\n",
    "        base_list = np.asarray(base_list)\n",
    "\n",
    "        # subtract baseline from average spectrum\n",
    "        subtract_baseline(measurement, x_key, y_key, new_key, base_list, base='polynomial', order=order,\n",
    "                          fixed_ends=True, show_plot=show_plot, save_plot=save_plot, debug=debug)\n",
    "\n",
    "        # baseline individual spectra if necessary\n",
    "        if measurement.points > 1:\n",
    "            if debug == True:\n",
    "                print(\"%s points:\" % title, measurement.points)\n",
    "                print(\"    map x,y arrays:\", np.shape(measurement['raman_shift']), np.shape(measurement['y']))\n",
    "            # subtract baseline from individual spectra\n",
    "            subtract_baseline(measurement, x_key, 'y', 'y_sub', base_list, base='poly', order=order,\n",
    "                              fixed_ends=True, show_plot=False, save_plot=False, debug=debug)\n",
    "            \n",
    "            if show_plot == True:\n",
    "                # plot average of baselines vs baseline of average\n",
    "                plt.figure(figsize=(8,4))\n",
    "                plt.title(\"%s:\\nAverage of %s baselined spectra\" % (title, measurement.points))\n",
    "                ### need to add st.dev. handling to make this work\n",
    "                ### plt.fill_between(spec['raman_shift'], np.mean(spec['y_sub'], axis=0)-np.std(spec['y_sub'],axis=0),\n",
    "                        ### np.mean(spec['y_sub'], axis=0)+np.std(spec['y_sub'], axis=0), color='k', alpha=0.1, linewidth=0.)\n",
    "                print(np.shape(measurement['y_sub'].y))\n",
    "                plt.plot(measurement['raman_shift'], measurement['y_sub'].mean(), 'k')\n",
    "                plt.xlim(measurement.x_start, measurement.x_end)\n",
    "                plt.show()\n",
    "\n",
    "            # get average of baselined spectra\n",
    "            log = measurement['y_sub'].log + ['averaged over %s spectra' % measurement.points]\n",
    "            measurement.add_spectrum(key='y_sub_av', y=measurement['y_sub'].mean(),\n",
    "                    label='Averaged Baselined Intensity (counts)', log=log)\n",
    "        process_count += 1\n",
    "        print(\"    baseline subtracted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reference Spectrum Subtraction\n",
    "- for specified spectra, subtract named reference spectrum\n",
    "- works by interpolating reference spectrum to match target spectrum, rescaling reference spectrum to fit target spectrum at key positions, then subtracting reference from target\n",
    "- requires processed reference spectrum to be in ./{Output_dir}/\n",
    "- y_av_sub is the baselined average spectrum\n",
    "- y_sub_av is the average of all baselined spectra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip this section?\n",
    "skip = False\n",
    "\n",
    "# print debug messages in viewer?\n",
    "debug = True\n",
    "\n",
    "# keyname of x value parameter to use ('raman_shift', 'wavelength', 'frequency')\n",
    "x_key = 'raman_shift'\n",
    "\n",
    "# keyname of y value parameter to use - defined for each reference in subtraction_refs dict\n",
    "\n",
    "# back-up keyname if y_key does not exist\n",
    "alt_key = 'y_av_sub'\n",
    "\n",
    "# keyname for new spectrum\n",
    "new_key = 'y_av_sub_refsub'\n",
    "\n",
    "# for each reference spectrum being used, add its spec ID to this dict as follows:\n",
    "# ref ID: {\n",
    "#   'name': 'glass', # name of reference\n",
    "#   'targets': [],  # list of spec IDs to subtract this reference from\n",
    "#   'x_list': []    # list of x positions to fit when rescaling reference to match y values of target\n",
    "# }\n",
    "subtraction_refs = {\n",
    "}\n",
    "\n",
    "# show plots in viewer?\n",
    "show_plot = True\n",
    "\n",
    "# save plots to file?\n",
    "save_plot = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if skip == True:\n",
    "    print(\"SKIPPING REF SPECTRA IMPORT\")\n",
    "else:\n",
    "    print(\"DOING REF SPECTRA IMPORT\")\n",
    "    \n",
    "    # data storage array for reference spectra\n",
    "    ref_data = {}\n",
    "    \n",
    "    # check which references need to be imported\n",
    "    refs_for_import = []\n",
    "    for ref, ref_info in subtraction_refs.items():\n",
    "        targets = ref_info['targets']\n",
    "        check = np.any(np.asarray([target in Spec_IDs for target in targets]))\n",
    "        if check == True:\n",
    "            refs_for_import.append(ref)\n",
    "\n",
    "    print()\n",
    "    print(\"references to import:\")\n",
    "    for ref_ID in refs_for_import:\n",
    "        print(\"    \", ref_ID)\n",
    "\n",
    "    # import processed reference spectra\n",
    "    for ref_ID in refs_for_import:\n",
    "        # get info from dict\n",
    "        ref_name = subtraction_refs[ref_ID]['name']\n",
    "        targets = [target for target in subtraction_refs[ref_ID]['targets'] if target in Spec_IDs]\n",
    "        x_list = subtraction_refs[ref_ID]['x_list']\n",
    "        rerun = subtraction_refs[ref_ID]['rerun_baseline']\n",
    "        add_poly = subtraction_refs[ref_ID]['add_polynomial']\n",
    "        poly_order = subtraction_refs[ref_ID]['poly_order']\n",
    "        key = subtraction_refs[ref_ID]['key']\n",
    "        print()\n",
    "        print(\"reference:\", ref_ID, ref_name)\n",
    "        print(\"    to be subtracted from:\", targets)\n",
    "        print(\"    x_list for fitting:\", x_list)\n",
    "        # check if ref is in imported data\n",
    "        success = False\n",
    "        if ref_ID in Spec_IDs:\n",
    "            # use currently-imported spectrum if possible\n",
    "            if hasattr(data[str(ref_ID)], 'y_av_sub'):\n",
    "                print(\"    using currently-imported data for %s\" % ref_ID)\n",
    "                # add link to data storage array\n",
    "                ref_data[str(ref_ID)] = data[str(ref_ID)]\n",
    "                ref_data[str(ref_ID)].name = ref_name\n",
    "                ref_data[str(ref_ID)].targets = targets\n",
    "                ref_data[str(ref_ID)].x_list = x_list\n",
    "                ref_data[str(ref_ID)].rerun_baseline = rerun\n",
    "                ref_data[str(ref_ID)].sub_key = key\n",
    "                ref_data[str(ref_ID)].add_polynomial = add_poly\n",
    "                ref_data[str(ref_ID)].poly_order = poly_order\n",
    "                success = True\n",
    "        if success == False:\n",
    "            # find output spectrum file instead\n",
    "            print(\"    searching for processed data file...\")\n",
    "            meta_dirs = glob.glob(\"%s*/*/*/%s_*_metadata.csv\" % (Ref_dir, ref_ID))\n",
    "            spec_dirs = glob.glob(\"%s*/*/*/%s_*_av-spectrum.csv\" % (Ref_dir, ref_ID))\n",
    "            print(\"        metadata files found:\", len(meta_dirs))\n",
    "            print(\"        spectrum files found:\", len(spec_dirs))\n",
    "\n",
    "            if len(meta_dirs) > 0 and len(spec_dirs) > 0:\n",
    "                # import spectrum file\n",
    "                meta = pd.read_csv(meta_dirs[0], index_col=0)\n",
    "                print(\"    imported metadata:\", meta.index.values)\n",
    "                spec = pd.read_csv(spec_dirs[0])\n",
    "                print(\"    imported spec array:\", np.shape(spec))\n",
    "                print(spec.info())\n",
    "                # convert metadata to dict for passing to a new Measurement\n",
    "                kwargs = {key: meta.loc[key][0] for key in meta.index.values}\n",
    "                # add any missing info required for a reference Measurement\n",
    "                kwargs['name'] = ref_name\n",
    "                kwargs['targets'] = targets\n",
    "                kwargs['targets'] = list(targets)\n",
    "                kwargs['x_list'] = x_list\n",
    "                kwargs['rerun_baseline'] = rerun\n",
    "                kwargs['add_polynomial'] = add_poly\n",
    "                kwargs['poly_order'] = poly_order\n",
    "                kwargs['sub_key'] = key\n",
    "                # add x,y data from spec\n",
    "                kwargs['x'] = spec['Raman Shift (cm-1)']\n",
    "                kwargs['y'] = spec['Baselined Intensity']\n",
    "                kwargs['ykey'] = 'y_av_sub'\n",
    "\n",
    "                # add to ref_data storage array\n",
    "                ref_data[str(ref_ID)] = Measurement(\n",
    "                    Fig_dir = Fig_dir,\n",
    "                    Out_dir = Out_dir,\n",
    "                    **kwargs\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if skip == True:\n",
    "    print(\"SKIPPING REFERENCE SPECTRUM SUBTRACTION\")\n",
    "else:\n",
    "    print(\"DOING REFERENCE SPECTRUM SUBTRACTION\")\n",
    "\n",
    "    if len(ref_data.keys()) == 0:\n",
    "        print()\n",
    "        print(\"no references imported!\")\n",
    "\n",
    "    # proceed with subtraction, ref by ref\n",
    "    for ref_ID, ref in ref_data.items():\n",
    "        # get ref info from dict\n",
    "        ref_name = ref.name\n",
    "        key = ref.sub_key\n",
    "        targets = ref.targets\n",
    "        x_list = ref.x_list\n",
    "        add_poly = ref.add_polynomial\n",
    "        poly_order = ref.poly_order\n",
    "        print()\n",
    "        print(\"reference %s (%s)...\" % (ref_ID, ref_name))\n",
    "        print(\"    applies to measurements:\", targets)\n",
    "        \n",
    "\n",
    "        # for each measurement in target list\n",
    "        process_count = 1\n",
    "        for target_ID in targets:\n",
    "            measurement = data[target_ID]\n",
    "            print()\n",
    "            print(\"    %s/%s subtracting %s from %s\" % (process_count, len(targets), ref_name, measurement.title))\n",
    "                \n",
    "            subtract_reference(ref, measurement, x_key, key, alt_key, new_key, x_list, add_polynomial=add_poly,\n",
    "                               poly_order=poly_order, plot=plot, show_plot=show_plot, debug=debug)\n",
    "\n",
    "            if ref.rerun_baseline == True:\n",
    "                # decide which set of baseline points to use\n",
    "                if measurement.laser_wavelength == 532:\n",
    "                    base_list = [200, 250, 300, 350, 400, 415, 500, 600, 700, 800, 900, 1050, 1150, 1300, 1400, 1600,\n",
    "                                 1700, 1800, 2000, 2100, 2300, 2500, 2700, 2950, 3100, 3200, 3700, 3800, 3900, 4000]\n",
    "                    order = 19\n",
    "                elif measurement.laser_wavelength in [633, 638]:\n",
    "                    base_list = [100, 200, 300, 400, 415, 500, 600, 700, 800, 900, 1050, 1150, 1400, 1500]\n",
    "                    order = 11\n",
    "                elif measurement.laser_wavelength == 785:\n",
    "                    base_list = [250, 350, 475, 525, 660, 790, 900, 950, 1000, 1200, 1300, 1700, 1900, 2000, 2250, 2500,\n",
    "                                     2600, 2950, 3100, 3500, 3600, 3800]\n",
    "                    order = 11\n",
    "                else:\n",
    "                    base_list = [100, 200, 300, 400, 500, 600, 700, 800, 900, 1050, 1150, 1400, 1500]\n",
    "                    order = 11\n",
    "                base_list = np.asarray(base_list)\n",
    "\n",
    "                # subtract baseline from average spectrum\n",
    "                subtract_baseline(measurement, x_key, new_key, new_key,\n",
    "                        base_list, base='polynomial', order=order, fixed_ends=True, plot=plot,\n",
    "                        show_plot=show_plot, save_plot=save_plot, plot_name='baselined2', debug=debug)\n",
    "            process_count += 1\n",
    "            print(\"        reference subtracted!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peak Detection\n",
    "- searches for local maxima that meet two thresholds:\n",
    "    1) relative intensity (vs maximum)\n",
    "    2) signal:noise ratio\n",
    "- produces figure showing which maxima passed and failed\n",
    "- also shows any manually specified maxima from Manual_Peaks[spec ID]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip this section?\n",
    "skip = False\n",
    "\n",
    "# print debug messages in viewer?\n",
    "debug = True\n",
    "\n",
    "# minimum signal:noise ratio threshold\n",
    "SNR_threshold=10\n",
    "\n",
    "# minimum intensity threshold relative to max\n",
    "norm_threshold=0.05\n",
    "\n",
    "# minimum peak-peak separation\n",
    "min_sep=20\n",
    "\n",
    "# keyname of x value parameter to use ('raman_shift', 'wavelength', 'frequency')\n",
    "x_key = 'raman_shift'\n",
    "\n",
    "# keyname of y value parameter to use\n",
    "y_key = 'y_av_sub_refsub'\n",
    "\n",
    "# back-up keyname if y_key does not exist\n",
    "alt_key = 'y_av_sub'\n",
    "\n",
    "# keyname for detected peak table\n",
    "new_key ='detected-peaks'\n",
    "\n",
    "# show plot in viewer?\n",
    "show_plot = True\n",
    "\n",
    "# save plots to file?\n",
    "save_plot = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if skip == True:\n",
    "    print(\"SKIPPING AUTOMATIC PEAK DETECTION\")\n",
    "else:\n",
    "    print(\"DOING AUTOMATIC PEAK DETECTION\")\n",
    "\n",
    "    process_count = 1\n",
    "    for ID, measurement in data.items():\n",
    "        print()\n",
    "        print(\"%s/%s detecting peaks for %s\" % (process_count, len(data.keys()), measurement.title))\n",
    "\n",
    "        if hasattr(measurement, y_key) == True:\n",
    "            temp = y_key\n",
    "        else:\n",
    "            temp = alt_key\n",
    "\n",
    "        detect_peaks(measurement, x_key, temp, new_key, SNR_threshold=SNR_threshold,\n",
    "                     norm_threshold=norm_threshold, min_sep=min_sep, show_plot=show_plot, save_plot=save_plot,\n",
    "                     debug=debug)\n",
    "        process_count += 1\n",
    "        print(\"    done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Peak Fitting\n",
    "- fits peaks using mathematical functions, choose from:\n",
    "    - PV: pseudo-voigt (linear mix of gaussian & lorentzian, recommended for most Raman spectra)\n",
    "    - G: gaussian (default)\n",
    "    - L: lorentzian\n",
    "    - FD: symmetric fermi-dirac (not recommended except in extreme cases)\n",
    "- will use manually specified peak positions for each spec ID in Manual_Peaks, or Default peak positions if listed. If no peaks are listed, uses peaks from peak detection instead\n",
    "- spectra will be divided into regions to be fitted separately, based on how far apart peaks are\n",
    "    - use the fit_window setting to change the max separation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip this section?\n",
    "skip = False\n",
    "\n",
    "# print debug messages in viewer?\n",
    "debug = False\n",
    "\n",
    "# function for peak fitting, choose from 'G', 'L', 'PV', or 'FD'\n",
    "Fit_Function = 'PV'\n",
    "\n",
    "# a list of peak positions to fit for each spec ID, leave empty to use automatically detected peaks\n",
    "Manual_Peaks = {\n",
    "}\n",
    "\n",
    "# keyname of x value parameter to use ('raman_shift', 'wavelength', 'frequency')\n",
    "x_key = 'raman_shift'\n",
    "\n",
    "# keyname of y value parameter to use\n",
    "y_key = 'y_av_sub_refsub'\n",
    "\n",
    "# back-up keyname if y_key does not exist\n",
    "alt_key = 'y_av_sub'\n",
    "\n",
    "# keyname for detected peak table\n",
    "new_key ='fitted-peaks'\n",
    "\n",
    "# show plot in viewer?\n",
    "show_plot = True\n",
    "\n",
    "# save plots to file?\n",
    "save_plot = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if skip == True:\n",
    "    print(\"SKIPPING AUTOMATIC PEAK FITTING\")\n",
    "else:\n",
    "    print(\"DOING AUTOMATIC PEAK FITTING\")\n",
    "\n",
    "    if len(Spec_IDs) == 0:\n",
    "        print(\"    no spectra imported, skipping\")\n",
    "\n",
    "    process_count = 1\n",
    "    for ID, measurement in data.items():\n",
    "        title = measurement.title\n",
    "        sample = measurement.sample\n",
    "        print()\n",
    "        print(\"%s/%s fitting peaks for %s\" % (process_count, len(data.keys()), measurement.title))\n",
    "\n",
    "        if hasattr(measurement, y_key) == True:\n",
    "            temp = y_key\n",
    "        else:\n",
    "            temp = alt_key\n",
    "\n",
    "        if ID in Manual_Peaks:\n",
    "            positions = Manual_Peaks[ID]\n",
    "            print(\"    specified peaks:\", \", \".join([\"%0.f\" % i for i in positions]))\n",
    "        elif hasattr(measurement, 'detected-peaks'):\n",
    "            positions = measurement['detected-peaks'].loc[:,'centers']\n",
    "            print(\"    detected peaks:\", \", \".join([\"%0.f\" % i for i in positions]))\n",
    "        else:\n",
    "            positions = []\n",
    "            print(\"    no peak positions specified\")\n",
    "\n",
    "        fit_peaks(measurement, x_key, temp, new_key, function=Fit_Function, peak_positions=positions,\n",
    "                  show_plot=show_plot, save_plot=save_plot, debug=debug)\n",
    "        process_count += 1\n",
    "        print(\"    done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Samples with the same label\n",
    "- this section groups spectra according to a specified property and plots comparison figures for each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip this section?\n",
    "skip = False\n",
    "\n",
    "# set whether to print debug messages in this section\n",
    "debug = False\n",
    "\n",
    "# keyname for x values to plot ('raman_shift', 'wavelength', 'frequency')\n",
    "x_key = 'raman_shift'\n",
    "\n",
    "# keyname of y value parameter to use\n",
    "y_key = 'y_av_sub_refsub'\n",
    "\n",
    "# back-up keyname if y_key does not exist\n",
    "alt_key = 'y_av_sub'\n",
    "\n",
    "# label for X axis\n",
    "x_label = \"Raman Shift (cm$^{-1}$)\"\n",
    "\n",
    "# label for Y axis\n",
    "y_label = 'Normalised Intensity'\n",
    "\n",
    "# X range for plotting\n",
    "x_start, x_end = (800, 1800)\n",
    "\n",
    "# normalise data before plotting?\n",
    "normalise = True\n",
    "\n",
    "# offset spectra by this much (0 for no offset)\n",
    "offset = 0.\n",
    "\n",
    "# group spectra by this parameter\n",
    "grouping = 'sample'\n",
    "\n",
    "# plot average spectra?\n",
    "plot_average = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if skip == True:\n",
    "    print(\"SKIPPING SAMPLE COMPARISON\")\n",
    "else:\n",
    "    print(\"PLOTTING SAMPLE COMPARISONS\")\n",
    "\n",
    "    # get groupings\n",
    "    groups = list(np.unique([measurement[grouping] for ID, measurement in data.items() if hasattr(measurement, grouping)]))\n",
    "    print()\n",
    "    print(\"%s groups found:\" % len(groups), groups)\n",
    "    \n",
    "    # plot spectra for each group separately\n",
    "    for group in groups:\n",
    "        result = [ID for ID, measurement in data.items() if getattr(measurement, grouping, \"\") == group]\n",
    "        spec_count = len(result)\n",
    "        print()\n",
    "        print(\"plotting group %s, %s spectra found\" % (group, spec_count))\n",
    "        if debug == True:\n",
    "            print(\"    \", result)\n",
    "        \n",
    "        count = 0\n",
    "\n",
    "        if offset != 0:\n",
    "            plt.figure(figsize=(8,2+len(result)))\n",
    "        else:\n",
    "            plt.figure(figsize=(8,4))\n",
    "        plt.title(\"Sample Spectra\\n%s\" % group)\n",
    "        plt.xlabel(x_label)\n",
    "        plt.xlim(x_start, x_end)\n",
    "        if offset > 0:\n",
    "            plt.ylabel(y_label)\n",
    "            plt.yticks([])\n",
    "        else:\n",
    "            plt.ylabel(y_label)\n",
    "\n",
    "        # plot average spectra\n",
    "        count = 0\n",
    "        if plot_average == True and spec_count > 1:\n",
    "            x, y = average_spectra([data[ID] for ID in result], x_key, y_key, alt_key, start=x_start, end=x_end,\n",
    "                                  normalise=normalise, debug=debug)\n",
    "            plt.plot(x, y - count*offset, 'k', label='mean', zorder=3)\n",
    "\n",
    "        # plot individual spectra\n",
    "        for ID in result:\n",
    "            # check which key to use\n",
    "            if hasattr(measurement, y_key) == True:\n",
    "                key = y_key\n",
    "            else:\n",
    "                key = alt_key\n",
    "            \n",
    "            # get data\n",
    "            x, y = get_plot_data(data[ID], x_key, key, start=x_start, end=x_end,\n",
    "                    normalise=normalise, debug=debug)\n",
    "            \n",
    "            # plot spectra\n",
    "            if len(result) < 10:\n",
    "                # plot individual spectra as distinct lines with their own labels\n",
    "                plt.plot(x, y - count*offset, Colour_List[count % len(Colour_List)],\n",
    "                         alpha=1./np.sqrt(spec_count), label=\"%s\" % (ID))\n",
    "            else:\n",
    "                # too many spectra to label individually, plot all spectra as semi-transparent lines of the same colour\n",
    "                plt.plot(x, y - count*offset, Colour_List[groups.index(group) % len(Colour_List)],\n",
    "                         alpha=1./np.sqrt(spec_count))\n",
    "            count += 1\n",
    "            \n",
    "        plt.legend(loc=1)\n",
    "        plt.minorticks_on()\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Samples with different labels\n",
    "- this section groups spectra according to a specified property and plots comparison figures for group averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip this section?\n",
    "skip = False\n",
    "\n",
    "# set whether to print debug messages in this section\n",
    "debug = False\n",
    "\n",
    "# keyname for x values to plot ('raman_shift', 'wavelength', 'frequency')\n",
    "x_key = 'raman_shift'\n",
    "\n",
    "# keyname for y values to plot\n",
    "y_key = 'y_av_sub_refsub'\n",
    "\n",
    "# keyname for y values to plot\n",
    "y_key = 'y_av_sub_refsub'\n",
    "\n",
    "# back-up keyname if y_key does not exist\n",
    "alt_key = 'y_av_sub'\n",
    "\n",
    "# label for X axis\n",
    "x_label = \"Raman Shift (cm$^{-1}$)\"\n",
    "\n",
    "# label for Y axis\n",
    "y_label = 'Normalised Intensity'\n",
    "\n",
    "# X range for plotting\n",
    "x_start, x_end = (800, 1800)\n",
    "\n",
    "# normalise data before plotting?\n",
    "normalise = True\n",
    "\n",
    "# offset spectra by this much (0 for no offset)\n",
    "offset = 0.\n",
    "\n",
    "# group spectra by this parameter\n",
    "grouping = 'sample'\n",
    "\n",
    "# plot average spectra?\n",
    "plot_average = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if skip == True:\n",
    "    print(\"SKIPPING SAMPLE COMPARISON\")\n",
    "else:\n",
    "    print(\"PLOTTING SAMPLE COMPARISONS\")\n",
    "\n",
    "    # get groupings\n",
    "    groups = list(np.unique([measurement[grouping] for ID, measurement in data.items() if hasattr(measurement, grouping)]))\n",
    "    print()\n",
    "    print(\"%s groups found:\" % len(groups), groups)\n",
    "        \n",
    "    if offset != 0:\n",
    "        plt.figure(figsize=(8,2+len(groups)))\n",
    "    else:\n",
    "        plt.figure(figsize=(8,4))\n",
    "    plt.title(\"Average Spectra\\nby %s\" % grouping)\n",
    "    plt.xlabel(x_label)\n",
    "    plt.xlim(x_start, x_end)\n",
    "    if offset != 0:\n",
    "        plt.ylabel(y_label)\n",
    "        plt.yticks([])\n",
    "    else:\n",
    "        plt.ylabel(y_label)\n",
    "        \n",
    "    count = 0\n",
    "\n",
    "    # plot spectra for each group separately\n",
    "    for group in groups:\n",
    "        result = [ID for ID, measurement in data.items() if getattr(measurement, grouping, \"\") == group]\n",
    "        spec_count = len(result)\n",
    "        \n",
    "        x, y = average_spectra([data[ID] for ID in result], x_key, y_key, alt_key, start=x_start, end=x_end,\n",
    "                    normalise=normalise, debug=debug)\n",
    "        plt.plot(x, y - count*offset, Colour_List[count % len(Colour_List)], label=group, zorder=3)\n",
    "        count += 1\n",
    "\n",
    "    plt.legend(loc=1)\n",
    "    plt.minorticks_on()\n",
    "    plt.tight_layout()\n",
    "    if save_plot == True:\n",
    "        plt.savefig(\"%s\")\n",
    "    if show_plot == True:\n",
    "        plt.show()\n",
    "    else:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Processed Spectra\n",
    "- for all measurements, the average spectrum is saved to _av-spectrum.csv\n",
    "    - includes columns for raw intensity, baselined intensity, and normalised values\n",
    "- for multi-spec measurements (e.g. maps and line-scans), all point-spectra are saved to _all-spectra_baselined.csv\n",
    "    - each spectrum is saved as a column along with its X,Y coordinates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# skip this section?\n",
    "skip = False\n",
    "\n",
    "# print debug messages in viewer?\n",
    "debug = False\n",
    "\n",
    "# save metadata to dedicated file? (recommended)\n",
    "save_metadata = True\n",
    "    \n",
    "if skip == True:\n",
    "    print(\"SKIPPING SAVE TO FILE STEP\")\n",
    "else:\n",
    "    print(\"SAVING PROCESSED SPECTRA TO OUTPUT FOLDER\")\n",
    "\n",
    "    process_count = 1\n",
    "    for ID, measurement in data.items():\n",
    "        title = measurement.title\n",
    "        print()\n",
    "        print(\"%s/%s saving spectrum %s\" % (process_count, len(data.keys()), measurement.title))\n",
    "\n",
    "        # one column for each modification of spectrum\n",
    "        headers = ['Wavelength (nm)', 'Raman Shift (cm-1)', 'Raw Intensity', 'Normalised Intensity',\n",
    "                   'Baselined Intensity', 'Normalised Baselined Intensity']\n",
    "        if hasattr(measurement, 'y_av_sub_refsub'):\n",
    "            print(\"    using y_av_sub_refsub\")\n",
    "            print(\"        log:\", \", \".join(measurement['y_av_sub_refsub'].log))\n",
    "            keys = ['wavelength', 'raman_shift', 'y_av', 'y_av_norm', 'y_av_sub_refsub', 'y_av_sub_refsub_norm']\n",
    "        else:\n",
    "            print(\"    using y_av_sub\")\n",
    "            keys = ['wavelength', 'raman_shift', 'y_av', 'y_av_norm', 'y_av_sub', 'y_av_sub_norm']\n",
    "        save_measurement(measurement, keys=keys, headers=headers, save_name='av-spectrum', debug=debug)\n",
    "\n",
    "        # save all point-spectra to file (maps & multi-spec files only)\n",
    "        if measurement.points > 1 and hasattr(measurement, 'y_sub'):\n",
    "            # one column per baselined point-spectrum\n",
    "            headers = ['Wavelength (nm)', 'Raman Shift (cm-1)', 'Baselined Intensity']\n",
    "            keys = ['wavelength', 'raman_shift', 'y_sub']\n",
    "            save_measurement(measurement, keys=keys, headers=headers, save_name='all-spectra-baselined',\n",
    "                             plot=True, show_plot=True, debug=debug)\n",
    "        process_count += 1\n",
    "        print(\"    saved!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"SCRIPT COMPLETE!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
