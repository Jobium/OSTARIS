"""
# ==================================================

This script takes processed Raman spectra and advanced analysis steps on them. Analysis steps include:
- Peak Detection
- Peak Fitting
        
Requires processed Raman spectrum files generated by the *_processing.py script
    - files should be stored in /{Output_dir}/{Sample}/{Laser Wavelength}/
    - individual measurements are always identified by their unique ID, the part of its filename before the first "_"
    - each measurement should have the following files:
        1) *_metadata.csv           (mandatory, contains all metadata for measurement)
        2) *_av-spectrum.csv        (mandatory, contains average spectrum data)
        3) *_all-spectra_*.csv      (optional, maps only, contains all point-spectra)
        4) *_xy-data.csv            (optional, maps only, contains XY coordinates for point-spectra)

# ==================================================
"""

# ==================================================
# import python modules

import os
import math
import glob
import datetime
import numpy as np
import pandas as pd
import lmfit as lmfit
import matplotlib as mpl
import matplotlib.pyplot as plt

from scipy.optimize import curve_fit
from scipy.signal import argrelextrema
from scipy.signal import savgol_filter

from functions_v01 import *

# ==================================================
# set initial variables and enable processing steps

Technique = 'Raman'         # 'Raman' or 'FTIR'

# filter data import by sample / subsample
Sample = '*'                # name of sample, or '*' for all
Subsample = '*'             # name of subsample, or '*'
Measurement_ID = '*'        # unique ID of measurement, or '*'
Measurement_Date = '*'      # Date in YYYY-MM-DD format as string, or '*'

# list spectral IDs to skip when importing data, as strings
Do_Not_Import = []

# filter by measurement settings
Laser_Wavelength = 785      # wavelength (in nm) as integer, or '*'
Laser_Power = '*'           # power (in mW or %) as int/float, or '*'
Exposure_Time = '*'         # exposure time (in sec) as int/float, or '*'
Accumulations = '*'         # accumulations as int, or '*'
Magnification = '*'         # objective magnification as int, or '*'
X_Start = '*'               # only import spectra that start at x <= X_Start
X_End = '*'                 # only import spectra that end at x >= X_End

# processes to run (see each section for more details)
Detect_Peaks = True            # detect peaks as maxima
Fit_Peaks = True               # fit peaks
Fit_Function = 'PV'             # function for peak fitting, choose from 'G', 'L', 'PV', or 'FD'
Manual_Peaks = {                # a list of peak positions to fit for each spec ID, leave empty to use automatically detected peaks
}

# list directories for input data, figures and output files
Data_dir = '../test data/'
Fig_dir = '../figures/'
Out_dir = '../output/'

# Colourblind-friendly palette developed by Paul Tol ( https://personal.sron.nl/~pault/#sec:qualitative )
Color_list_dark = ['#4477AA', '#EE6677', '#228833', '#CCBB44', '#66CCEE', '#AA3377', '#BBBBBB']
Color_list_light = ['#77AADD', '#EE8866', '#EEDD88', '#FFAABB', '#99DDFF', '#44BB99', '#BBCC33', '#AAAA00', '#DDDDDD']
Color_list = Color_list_dark + Color_list_light

"""
# ==================================================
# FILE SEARCH
# - this section searches for spectrum files that match the specified settings
# ==================================================
"""

print()
print()
print("SEARCHING FOR PROCESSED DATA FILES...")

# find unique spec IDs in output folder
text = "%s%s/%snm/*/%s_*_metadata.csv" % (Out_dir, Sample, str(Laser_Wavelength), Measurement_ID)
metadata_dirs = glob.glob(text)
Spec_IDs = np.unique([filedir.split("/")[-1].split("_")[0] for filedir in metadata_dirs])
print(text)
print(Spec_IDs)

# import metadata for each file for comparison to user-defined setting filters
count = 0
for filedir in metadata_dirs:
    ID = filedir.split("/")[-1].split("_")[0]
    if count == 0:
        metadata = pd.read_csv(filedir, index_col='Parameter', names=['Parameter', ID], header=0)
    else:
        metadata = pd.concat([metadata, pd.read_csv(filedir, index_col='Parameter', names=['Parameter', ID], header=0)], axis=1)
    count += 1

print()
print("%s metadata files found" % np.shape(metadata)[1])

print(metadata.info())

# map user-defined settings to metadata row labels
params = {'ID': Measurement_ID, 'sample': Sample, 'subsample': Subsample, 'measurement_date': Measurement_Date, 'laser_wavelength': str(Laser_Wavelength), 'laser_power': Laser_Power, 'accumulations': Accumulations, 'exposure_time': Exposure_Time, 'magnification': Magnification, 'technique': Technique, 'x_start': X_Start, 'x_end': X_End}

print()
print(params.items())

# filter based on user-defined settings
check = []
for key, value in params.items():
    if value != '*':
        # if setting has a defined value
        if key in ['x_start']:
            # must be equal to or less than value
            check.append(metadata.loc[key].values <= value)
        if key in ['x_end']:
            # must be equal to or greater than value
            check.append(metadata.loc[key].values >= value)
        else:
            # must be exactly equal to value
            check.append(metadata.loc[key].values == value)
check = np.all(np.asarray(check), axis=0)
print()
print("%s files match specified settings" % np.count_nonzero(check))

Spec_IDs = metadata.columns.values[check]

print("    IDs:", Spec_IDs)

"""
# ==================================================
# DATA IMPORT
# - this section actually imports data files along with metadata
# ==================================================
"""

print()
print()
print("IMPORTING SPECTRA...")

# set up data storage dictionary
data = {}

# ==================================================
# each measurement imported will be added to this dictionary as a Measurement object. To access a particular measurement, use data[measurement ID].

# for each unique ID that matched filters
for ID in Spec_IDs:
    while True:
        try:
            print()
            print("attempting to import data for %s" % ID)
            
            # convert metadata into kwargs
            meta_temp = dict(metadata[ID])
            # convert 
            print("        title:", meta_temp['title'])
            print("       sample:", meta_temp['sample'])
            print("    subsample:", meta_temp['subsample'])
            print("    technique:", meta_temp['technique'])
            # get output dir from metadata
            print("     data dir:", meta_temp['out_dir'])
            
            # check for available files
            avspec_dir = glob.glob("%s%s_*_av-spectrum.csv" % (meta_temp['out_dir'], ID))
            mapspec_dir = glob.glob("%s%s_*_all-spectra-baselined.csv" % (meta_temp['out_dir'], ID))
            xy_dir = glob.glob("%s%s_*_xycoords.csv" % (meta_temp['out_dir'], ID))
            peakfit_dir = glob.glob("%s%s_*_fitted-peaks.csv" % (meta_temp['out_dir'], ID))
            print()
            print("    average spectrum files found:", len(avspec_dir))
            print("    spectrum map files found:", len(avspec_dir))
            print("    x,y coordinate files found:", len(xy_dir))
            print("    fitted peak files found:", len(peakfit_dir))
            
            if len(avspec_dir) == 0:
                raise Exception("    cannot import %s without x,y data from av-spectrum.csv!" % ID)
                
            # extract average spectrum
            spec = pd.read_csv(avspec_dir[0])
            print()
            print("    average spectrum array:", np.shape(spec))
            if meta_temp['technique'] == 'Photoluminescence':
                # import spectrum as raw intensity vs wavelength
                x = spec['Wavelength (nm)']
                y = spec['Raw Intensity']
            elif meta_temp['technique'] == 'Raman':
                # import spectrum as baselined intensity vs raman shift
                x = spec['Raman Shift (cm-1)']
                y = spec['Baselined Intensity']
            else:
                # import spectrum as absorbance vs frequency
                x = spec['Frequency (cm-1)']
                y = spec['Absorbance (arb. u.)']
            print("        x,y arrays:", np.shape(x), np.shape(y))
            print("        x range: %0.1f - %0.1f" % (np.amin(x), np.amax(x)))
            print("        y range: %0.1f - %0.1f" % (np.amin(y), np.amax(y)))
            
            print("    creating Measurement object for %s" % ID)
            # create Measurement object
            data[str(ID)] = Measurement(
                Fig_dir = Fig_dir,
                Out_dir = Out_dir,
                x = x,
                y = y,
                ykey = 'y_av_sub',
                ylabel = 'Baselined Intensity (counts)',
                **meta_temp
            )
            
            # import map spectra if available
            if len(mapspec_dir) > 0:
                spec = pd.read_csv(mapspec_dir[0])
                print()
                print("    spectral map array:", np.shape(spec))
                cols = [s for s in spec.columns.values if s not in ['Wavelength (nm)', 'Frequency (cm-1)', 'Raman Shift (cm-1)']]
                if meta_temp['technique'] == 'Photoluminescence':
                    # import spectrum as raw intensity vs wavelength
                    x = spec['Wavelength (nm)']
                elif meta_temp['technique'] == 'Raman':
                    # import spectrum as baselined intensity vs raman shift
                    x = spec['Raman Shift (cm-1)']
                else:
                    # import spectrum as absorbance vs frequency
                    x = spec['Frequency (cm-1)']
                if np.any(x != data[str(ID)].x):
                    raise Exception("imported x values in map file do not match values in av-spectrum file!")
                y = np.asarray(spec.loc[:, cols])
                print("        map contains %s point-spectra" % len(cols))
                print("        x,y arrays:", np.shape(x), np.shape(y))
                print("        x range: %0.1f - %0.1f" % (np.amin(x), np.amax(x)))
                print("        y range: %0.1f - %0.1f" % (np.amin(y), np.amax(y)))
                
                print("            adding map to Measurement object")
                data[str(ID)].add_spectrum('y_sub', y, label='Baselined Intensity (counts)')
                
            # import x,y coordinate data if available
            if len(xy_dir) > 0:
                table = pd.read_csv(xy_dir[0])
                data[str(ID)].x_coords = np.asarray(table['X'])
                data[str(ID)].y_coords = np.asarray(table['Y'])
                
            # import peak fit data if available
            if len(peakfit_dir) > 0:
                table = pd.read_csv(peakfit_dir[0])
                data[str(ID)].fitted_peaks = table
                
            print()
            print("    successfully imported data for %s!" % ID)
            break
        except Exception as e:
            print("    something went wrong! Exception:", e)
            break
            
print()
print("%s/%s files imported" % (len(data.keys()), len(Spec_IDs)))

# report which files were imported and which were not
print()
for num in Spec_IDs:
    if num in data.keys():
        print(num, u'\u2713', data[num]['filename'])
    else:
        text = ''
        if num in Do_Not_Import:
            text = 'ID in Do_Not_Import list'
        print(num, 'X', text)

# update list of Spec IDs to only include imported spectra
Spec_IDs = list(data.keys())

samples = np.unique([data[num]['sample'] for num in Spec_IDs])
print()
print("samples in dataset:", samples)

lasers = np.unique([measurement.laser_wavelength for ID, measurement in data.items()])
print()
print("laser wavelengths in dataset:", lasers)
            
"""
# ==================================================
# PEAK DETECTION
# - searches for local maxima that meet two thresholds:
#       1) relative intensity (vs maximum)
#       2) signal:noise ratio
# - produces figure showing which maxima passed and failed
# - also shows any manually specified maxima from Manual_Peaks[spec ID]
# ==================================================
"""

# set whether to print debug messages in this section
debug = True

if Detect_Peaks == True:
    print()
    print()
    print("DOING AUTOMATIC PEAK DETECTION")

    if len(Spec_IDs) == 0:
        print("    no spectra imported, skipping")

    for ID, measurement in data.items():
        title = measurement.title
        sample = measurement.sample
        print()
        print(ID, title)
        
        detect_peaks(measurement, 'raman_shift', 'y_av_sub', 'detected-peaks', plot=True, debug=debug)
    
"""
# ==================================================
# PEAK FITTING
# - fits peaks using mathematical functions, choose from:
#       - PV: pseudo-voigt (linear mix of gaussian & lorentzian, recommended for most Raman spectra)
#       - G: gaussian (default)
#       - L: lorentzian
#       - FD: symmetric fermi-dirac (not recommended except in extreme cases)
# - will use manually specified peak positions for each spec ID in Manual_Peaks, or Default peak positions if listed. If no peaks are listed, uses peaks from peak detection instead
# - spectra will be divided into regions to be fitted separately, based on how far apart peaks are
#       - use the fit_window setting to change the max separation
# ==================================================
"""

# set whether to print debug messages in this section
debug = True

if Fit_Peaks == True:
    print()
    print()
    print("DOING AUTOMATIC PEAK FITTING")

    if len(Spec_IDs) == 0:
        print("    no spectra imported, skipping")

    for ID, measurement in data.items():
        title = measurement.title
        sample = measurement.sample
        print()
        print(ID, title)
        
        fit_peaks(measurement, 'raman_shift', 'y_av_sub', 'fitted-peaks', plot=True, debug=debug)
        
"""
# ==================================================
# end of script
# ==================================================
"""

print()
print()
print("DONE")